package io.github.datacatering.datacaterer.core.generator

import io.github.datacatering.datacaterer.api.FieldBuilder
import io.github.datacatering.datacaterer.api.model.Constants.{ALL_COMBINATIONS, OMIT, ONE_OF_GENERATOR, REGEX_GENERATOR}
import io.github.datacatering.datacaterer.api.model.{Count, DoubleType, Field, IntegerType, PerFieldCount, Step}
import io.github.datacatering.datacaterer.core.util.{Account, SparkSuite}
import net.datafaker.Faker
import org.apache.spark
import org.apache.spark.sql.{Dataset, Encoder, Encoders, Row}

class DataGeneratorFactoryTest extends SparkSuite {

  private val dataGeneratorFactory = new DataGeneratorFactory(new Faker() with Serializable)
  private val fields = List(
    FieldBuilder().name("id").minLength(20).maxLength(25),
    FieldBuilder().name("amount").`type`(DoubleType).min(0.0).max(1000.0),
    FieldBuilder().name("debit_credit").oneOf(List("D", "C")),
    FieldBuilder().name("name").regex("[A-Z][a-z]{2,6} [A-Z][a-z]{2,8}"),
    FieldBuilder().name("code").`type`(IntegerType).sql("CASE WHEN debit_credit == 'D' THEN 1 ELSE 0 END"),
    FieldBuilder().name("party_id").uuid().incremental(),
    FieldBuilder().name("customer_id").`type`(IntegerType).incremental(),
  ).map(_.field) ++
    FieldBuilder().name("rank").oneOfWeighted((1, 0.8), (2, 0.1), (3, 0.1)).map(_.field) ++
    FieldBuilder().name("rating").oneOfWeighted(("A", 1), ("B", 2), ("C", 3)).map(_.field)

  private val simpleFields = List(Field("id"), Field("name"))

  private val nestedFields = List(
    Field("id"),
    Field("tmp_account_id", Some("string"), Map(REGEX_GENERATOR -> "ACC[0-9]{8}", OMIT -> "true")),
    Field("details", fields = List(Field("account_id", Some("string"), Map("sql" -> "tmp_account_id"))))
  )

  test("Can generate data for basic step") {
    val step = Step("transaction", "parquet", Count(records = Some(10)), Map("path" -> "sample/output/parquet/transactions"), fields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assertResult(10L)(df.count())
    assertResult(Array("id", "amount", "debit_credit", "name", "code", "party_id", "customer_id", "rank_weight", "rank", "rating_weight", "rating"))(df.columns)
    assertResult(Array(
      ("id", spark.sql.types.StringType),
      ("amount", spark.sql.types.DoubleType),
      ("debit_credit", spark.sql.types.StringType),
      ("name", spark.sql.types.StringType),
      ("code", spark.sql.types.IntegerType),
      ("party_id", spark.sql.types.StringType),
      ("customer_id", spark.sql.types.IntegerType),
      ("rank_weight", spark.sql.types.DoubleType),
      ("rank", spark.sql.types.IntegerType),
      ("rating_weight", spark.sql.types.DoubleType),
      ("rating", spark.sql.types.StringType),
    ))(df.schema.fields.map(x => (x.name, x.dataType)))
    val rows = df.collect()
    val sampleRow = df.head()
    assert(sampleRow.getString(0).nonEmpty && sampleRow.getString(0).length >= 20)
    assert(sampleRow.getDouble(1) >= 0.0)
    val debitCredit = sampleRow.getString(2)
    assert(debitCredit == "D" || debitCredit == "C")
    assert(sampleRow.getString(3).matches("[A-Z][a-z]{2,6} [A-Z][a-z]{2,8}"))
    if (debitCredit == "D") assert(sampleRow.getInt(4) == 1) else assert(sampleRow.getInt(4) == 0)
    assertResult("c4ca4238-a0b9-2382-0dcc-509a6f75849b")(sampleRow.getString(5))
    rows.foreach(row => {
      val customerId = row.getInt(6)
      assert(customerId > 0 && customerId <= 10)
      val rank = row.getInt(8)
      assert(rank == 1 || rank == 2 || rank == 3)
      val rating = row.getString(10)
      assert(rating == "A" || rating == "B" || rating == "C")
    })
  }

  test("Can generate data when number of rows per field is defined") {
    val step = Step("transaction", "parquet",
      Count(records = Some(10), perField = Some(PerFieldCount(List("id"), Some(2)))),
      Map("path" -> "sample/output/parquet/transactions"), simpleFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assertResult(20L)(df.count())
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assertResult(2L)(sampleRows.count())
  }

  test("Can generate data with generated number of rows per field by a generator") {
    val step = Step("transaction", "parquet", Count(Some(10),
      perField = Some(PerFieldCount(List("id"), None, Map("min" -> "1", "max" -> "2")))),
      Map("path" -> "sample/output/parquet/transactions"), simpleFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assert(df.count() >= 10L)
    assert(df.count() <= 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assert(sampleRows.count() >= 1L)
    assert(sampleRows.count() <= 2L)
  }

  test("Can generate data with generated number of rows generated by a data generator") {
    val step = Step("transaction", "parquet", Count(None,
      perField = None,
      options = Map("min" -> "10", "max" -> "20")),
      Map("path" -> "sample/output/parquet/transactions"), simpleFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 15)
    df.cache()

    assert(df.count() >= 10L)
    assert(df.count() <= 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assertResult(1L)(sampleRows.count())
  }

  test("Can generate data with all possible oneOf combinations enabled in step") {
    val step = Step("transaction", "parquet", Count(),
      Map("path" -> "sample/output/parquet/transactions", ALL_COMBINATIONS -> "true"), fields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 15)
    df.cache()

    assertResult(2L)(df.count())
    val idx = df.columns.indexOf("debit_credit")
    assert(df.collect().exists(r => r.getString(idx) == "D"))
    assert(df.collect().exists(r => r.getString(idx) == "C"))
  }

  test("Can generate data with all possible oneOf combinations enabled in step with multiple oneOf fields") {
    val statusField = Field("status", Some("string"),
      Map(ONE_OF_GENERATOR -> List("open", "closed", "suspended")))
    val fieldsWithStatus = fields ++ List(statusField)
    val step = Step("transaction", "parquet", Count(),
      Map("path" -> "sample/output/parquet/transactions", ALL_COMBINATIONS -> "true"), fieldsWithStatus)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 15)
    df.cache()

    assertResult(6L)(df.count())
    val debitIdx = df.columns.indexOf("debit_credit")
    val statusIdx = df.columns.indexOf("status")
    assertResult(3)(df.collect().count(r => r.getString(debitIdx) == "D"))
    assertResult(3)(df.collect().count(r => r.getString(debitIdx) == "C"))
    assertResult(2)(df.collect().count(r => r.getString(statusIdx) == "open"))
    assertResult(2)(df.collect().count(r => r.getString(statusIdx) == "closed"))
    assertResult(2)(df.collect().count(r => r.getString(statusIdx) == "suspended"))
  }

  test("Can generate data with nested field part of per field count") {
    val step = Step("transaction", "parquet", Count(Some(10),
      perField = Some(PerFieldCount(List("tmp_account_id"), Some(2)))),
      Map("path" -> "sample/output/parquet/transactions"), nestedFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assertResult(20L)(df.count())
    val dfArr = df.collect()
    dfArr.foreach(row => {
      val sampleId = row.getAs[Row]("details").getAs[String]("account_id")
      val sampleRows = df.filter(_.getAs[Row]("details").getAs[String]("account_id") == sampleId)
      assert(sampleRows.count() == 2L)
    })
  }

  ignore("Can run spark streaming output at 2 records per second") {
    implicit val encoder: Encoder[Account] = Encoders.kryo[Account]
    val df = sparkSession.readStream
      .format("rate").option("rowsPerSecond", "10").load()
      .map(_ => Account())
      .limit(100)
    val stream = df.writeStream
      .foreachBatch((batch: Dataset[_], id: Long) => println(s"batch-id=$id, size=${batch.count()}"))
      .start()
    stream.awaitTermination(11000)
  }
}
