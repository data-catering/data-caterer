package io.github.datacatering.datacaterer.core.generator

import io.github.datacatering.datacaterer.api.model.Constants.{ALL_COMBINATIONS, MAXIMUM_LENGTH, MINIMUM_LENGTH, OMIT, ONE_OF_GENERATOR, REGEX_GENERATOR}
import io.github.datacatering.datacaterer.api.model.{Count, Field, PerFieldCount, Step}
import io.github.datacatering.datacaterer.core.util.{Account, SparkSuite}
import net.datafaker.Faker
import org.apache.spark.sql.types.{DoubleType, IntegerType, StringType}
import org.apache.spark.sql.{Dataset, Encoder, Encoders, Row}
import org.junit.runner.RunWith
import org.scalatestplus.junit.JUnitRunner

@RunWith(classOf[JUnitRunner])
class DataGeneratorFactoryTest extends SparkSuite {

  private val dataGeneratorFactory = new DataGeneratorFactory(new Faker() with Serializable)
  private val fields = List(
    Field("id", Some("string"), Map(MINIMUM_LENGTH -> "20", MAXIMUM_LENGTH -> "25")),
    Field("amount", Some("double")),
    Field("debit_credit", Some("string"), Map(ONE_OF_GENERATOR -> List("D", "C"))),
    Field("name", Some("string"), Map(REGEX_GENERATOR -> "[A-Z][a-z]{2,6} [A-Z][a-z]{2,8}")),
    Field("code", Some("int"), Map("sql" -> "CASE WHEN debit_credit == 'D' THEN 1 ELSE 0 END")),
  )

  private val simpleFields = List(Field("id"), Field("name"))

  private val nestedFields = List(
    Field("id"),
    Field("tmp_account_id", Some("string"), Map(REGEX_GENERATOR -> "ACC[0-9]{8}", OMIT -> "true")),
    Field("details", fields = List(Field("account_id", Some("string"), Map("sql" -> "tmp_account_id"))))
  )

  test("Can generate data for basic step") {
    val step = Step("transaction", "parquet", Count(records = Some(10)), Map("path" -> "sample/output/parquet/transactions"), fields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assertResult(10L)(df.count())
    assert(df.columns sameElements Array("id", "amount", "debit_credit", "name", "code"))
    assert(df.schema.fields.map(x => (x.name, x.dataType)) sameElements Array(
      ("id", StringType),
      ("amount", DoubleType),
      ("debit_credit", StringType),
      ("name", StringType),
      ("code", IntegerType),
    ))
    val sampleRow = df.head()
    assert(sampleRow.getString(0).nonEmpty && sampleRow.getString(0).length >= 20)
    assert(sampleRow.getDouble(1) >= 0.0)
    val debitCredit = sampleRow.getString(2)
    assert(debitCredit == "D" || debitCredit == "C")
    assert(sampleRow.getString(3).matches("[A-Z][a-z]{2,6} [A-Z][a-z]{2,8}"))
    if (debitCredit == "D") assert(sampleRow.getInt(4) == 1) else assert(sampleRow.getInt(4) == 0)
  }

  test("Can generate data when number of rows per field is defined") {
    val step = Step("transaction", "parquet",
      Count(records = Some(10), perField = Some(PerFieldCount(List("id"), Some(2)))),
      Map("path" -> "sample/output/parquet/transactions"), simpleFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assertResult(20L)(df.count())
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assertResult(2L)(sampleRows.count())
  }

  test("Can generate data with generated number of rows per field by a generator") {
    val step = Step("transaction", "parquet", Count(Some(10),
      perField = Some(PerFieldCount(List("id"), None, Map("min" -> "1", "max" -> "2")))),
      Map("path" -> "sample/output/parquet/transactions"), simpleFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assert(df.count() >= 10L)
    assert(df.count() <= 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assert(sampleRows.count() >= 1L)
    assert(sampleRows.count() <= 2L)
  }

  test("Can generate data with generated number of rows generated by a data generator") {
    val step = Step("transaction", "parquet", Count(None,
      perField = None,
      options = Map("min" -> "10", "max" -> "20")),
      Map("path" -> "sample/output/parquet/transactions"), simpleFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 15)
    df.cache()

    assert(df.count() >= 10L)
    assert(df.count() <= 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assertResult(1L)(sampleRows.count())
  }

  test("Can generate data with all possible oneOf combinations enabled in step") {
    val step = Step("transaction", "parquet", Count(),
      Map("path" -> "sample/output/parquet/transactions", ALL_COMBINATIONS -> "true"), fields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 15)
    df.cache()

    assertResult(2L)(df.count())
    val idx = df.columns.indexOf("debit_credit")
    assert(df.collect().exists(r => r.getString(idx) == "D"))
    assert(df.collect().exists(r => r.getString(idx) == "C"))
  }

  test("Can generate data with all possible oneOf combinations enabled in step with multiple oneOf fields") {
    val statusField = Field("status", Some("string"),
      Map(ONE_OF_GENERATOR -> List("open", "closed", "suspended")))
    val fieldsWithStatus = fields ++ List(statusField)
    val step = Step("transaction", "parquet", Count(),
      Map("path" -> "sample/output/parquet/transactions", ALL_COMBINATIONS -> "true"), fieldsWithStatus)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 15)
    df.cache()

    assertResult(6L)(df.count())
    val debitIdx = df.columns.indexOf("debit_credit")
    val statusIdx = df.columns.indexOf("status")
    assertResult(3)(df.collect().count(r => r.getString(debitIdx) == "D"))
    assertResult(3)(df.collect().count(r => r.getString(debitIdx) == "C"))
    assertResult(2)(df.collect().count(r => r.getString(statusIdx) == "open"))
    assertResult(2)(df.collect().count(r => r.getString(statusIdx) == "closed"))
    assertResult(2)(df.collect().count(r => r.getString(statusIdx) == "suspended"))
  }

  test("Can generate data with nested field part of per field count") {
    val step = Step("transaction", "parquet", Count(Some(10),
      perField = Some(PerFieldCount(List("tmp_account_id"), Some(2)))),
      Map("path" -> "sample/output/parquet/transactions"), nestedFields)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet", 0, 10)
    df.cache()

    assertResult(20L)(df.count())
    val dfArr = df.collect()
    dfArr.foreach(row => {
      val sampleId = row.getAs[Row]("details").getAs[String]("account_id")
      val sampleRows = df.filter(_.getAs[Row]("details").getAs[String]("account_id") == sampleId)
      assert(sampleRows.count() == 2L)
    })
  }

  ignore("Can run spark streaming output at 2 records per second") {
    implicit val encoder: Encoder[Account] = Encoders.kryo[Account]
    val df = sparkSession.readStream
      .format("rate").option("rowsPerSecond", "10").load()
      .map(_ => Account())
      .limit(100)
    val stream = df.writeStream
      .foreachBatch((batch: Dataset[_], id: Long) => println(s"batch-id=$id, size=${batch.count()}"))
      .start()
    stream.awaitTermination(11000)
  }
}
