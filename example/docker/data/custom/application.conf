flags {
    enableCount = true
    enableCount = ${?ENABLE_COUNT}
    enableGenerateData = true
    enableGenerateData = ${?ENABLE_GENERATE_DATA}
    enableGeneratePlanAndTasks = true
    enableGeneratePlanAndTasks = ${?ENABLE_GENERATE_PLAN_AND_TASKS}
    enableRecordTracking = false
    enableRecordTracking = ${?ENABLE_RECORD_TRACKING}
    enableDeleteGeneratedRecords = false
    enableDeleteGeneratedRecords = ${?ENABLE_DELETE_GENERATED_RECORDS}
    enableFailOnError = true
    enableFailOnError = ${?ENABLE_FAIL_ON_ERROR}
    enableUniqueCheck = true
    enableUniqueCheck = ${?ENABLE_UNIQUE_CHECK}
    enableSinkMetadata = false
    enableSinkMetadata = ${?ENABLE_SINK_METADATA}
    enableSaveReports = true
    enableSaveReports = ${?ENABLE_SAVE_REPORTS}
    enableValidation = false
    enableValidation = ${?ENABLE_VALIDATION}
    enableGenerateValidations = false
    enableGenerateValidations = ${?ENABLE_GENERATE_VALIDATIONS}
    enableAlerts = false
    enableAlerts = ${?ENABLE_ALERTS}
    enableUniqueCheckOnlyInBatch = false
    enableUniqueCheckOnlyInBatch = ${?ENABLE_UNIQUE_CHECK_ONLY_IN_BATCH}
}

folders {
    generatedPlanAndTaskFolderPath = "/opt/app/custom/generated"
    generatedPlanAndTaskFolderPath = ${?GENERATED_PLAN_AND_TASK_FOLDER_PATH}
    planFilePath = "/opt/app/custom/plan/postgres-plan.yaml"
    planFilePath = ${?PLAN_FILE_PATH}
    taskFolderPath = "/opt/app/custom/task"
    taskFolderPath = ${?TASK_FOLDER_PATH}
    validationFolderPath = "/opt/app/custom/validation/csv"
    validationFolderPath = ${?VALIDATION_FOLDER_PATH}
    recordTrackingFolderPath = "/opt/app/custom/recordTracking"
    recordTrackingFolderPath = ${?RECORD_TRACKING_FOLDER_PATH}
    recordTrackingForValidationFolderPath = "/opt/app/custom/validation/recordTracking"
    recordTrackingForValidationFolderPath = ${?RECORD_TRACKING_VALIDATION_FOLDER_PATH}
    generatedReportsFolderPath = "/opt/app/custom/report"
    generatedReportsFolderPath = ${?GENERATED_REPORTS_FOLDER_PATH}
}

metadata {
    numRecordsFromDataSource = 10000
    numRecordsFromDataSource = ${?NUM_RECORDS_FROM_DATA_SOURCE}
    numRecordsForAnalysis = 10000
    numRecordsForAnalysis = ${?NUM_RECORDS_FOR_ANALYSIS}
    oneOfDistinctCountVsCountThreshold = 0.1
    oneOfDistinctCountVsCountThreshold = ${?ONE_OF_DISTINCT_COUNT_VS_COUNT_THRESHOLD}
    oneOfMinCount = 1000
    oneOfMinCount = ${?ONE_OF_MIN_COUNT}
    numGeneratedSamples = 10
    numGeneratedSamples = ${?NUM_GENERATED_SAMPLES}
}

generation {
    numRecordsPerBatch = 1000000
    numRecordsPerBatch = ${?NUM_RECORDS_PER_BATCH}
}

validation {
    numSampleErrorRecords = 5
    numSampleErrorRecords = ${?NUM_SAMPLE_ERROR_RECORDS}
    enableDeleteRecordTrackingFiles = true
    enableDeleteRecordTrackingFiles = ${?ENABLE_DELETE_RECORD_TRACKING_FILES}
}

alert {
    triggerOn = "all"
    triggerOn = ${?ALERT_TRIGGER_ON}
    slackAlertConfig {
        token = ""
        token = ${?ALERT_SLACK_TOKEN}
        channels = []
        channels = ${?ALERT_SLACK_CHANNELS}
    }
}

runtime {
    master = "local[*]"
    master = ${?DATA_CATERER_MASTER}
    config {
        "spark.driver.memory" = "6g",
        "spark.executor.memory" = "6g",
        "spark.executor.memoryOverhead" = "512m",
        "spark.memory.fraction" = "0.6",
        "spark.memory.storageFraction" = "0.5",
        "spark.memory.offHeap.size" = "1g",
        "spark.sql.shuffle.partitions" = "10",
        "spark.sql.cbo.enabled" = "true",
        "spark.sql.adaptive.enabled" = "true",
        "spark.sql.cbo.planStats.enabled" = "true",
        "spark.sql.legacy.allowUntypedScalaUDF" = "true",
        "spark.sql.legacy.allowParameterlessCount" = "true",
        "spark.sql.statistics.histogram.enabled" = "true",
        "spark.sql.catalog.postgres" = "",
        "spark.sql.catalog.cassandra" = "com.datastax.spark.connector.datasource.CassandraCatalog",
        "spark.sql.catalog.iceberg" = "org.apache.iceberg.spark.SparkCatalog",
        "spark.sql.catalog.iceberg.type" = "hadoop",
        "spark.hadoop.fs.s3a.directory.marker.retention" = "keep",
        "spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled" = "true",
        "spark.hadoop.fs.hdfs.impl" = "org.apache.hadoop.hdfs.DistributedFileSystem",
        "spark.hadoop.fs.file.impl" = "com.globalmentor.apache.hadoop.fs.BareLocalFileSystem",
        "spark.sql.extensions" = "io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    }
}

json {
    json {
    }
}

csv {
    csv {
    }
}

delta {
    delta {
    }
}

iceberg {
    iceberg {
    }
}

orc {
    orc {
    }
}

parquet {
    parquet {
    }
}

jdbc {
    postgresCustomer {
        url = "jdbc:postgresql://postgresserver:5432/customer"
        url = ${?POSTGRES_URL}
        user = "postgres"
        user = ${?POSTGRES_USER}
        password = "postgres"
        password = ${?POSTGRES_PASSWORD}
        driver = "org.postgresql.Driver"
    }
    mysql {
        url = "jdbc:mysql://mysqlserver:3306/customer"
        url = ${?MYSQL_URL}
        user = "root"
        user = ${?MYSQL_USERNAME}
        password = "root"
        password = ${?MYSQL_PASSWORD}
        driver = "com.mysql.cj.jdbc.Driver"
    }
}


org.apache.spark.sql.cassandra {
    cassandra {
        spark.cassandra.connection.host = "cassandraserver"
        spark.cassandra.connection.host = ${?CASSANDRA_HOST}
        spark.cassandra.connection.port = "9042"
        spark.cassandra.connection.port = ${?CASSANDRA_PORT}
        spark.cassandra.auth.username = "cassandra"
        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}
        spark.cassandra.auth.password = "cassandra"
        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}
    }
}

http {
    httpbin {
    }
}

jms {
    customer_rabbitmq {
        connectionFactory = "com.rabbitmq.jms.admin.RMQConnectionFactory"
        connectionFactory = ${?RABBITMQ_CONNECTION_FACTORY}
        url = "amqp://localhost:5672"
        url = ${?RABBITMQ_URL}
        user = "guest"
        user = ${?RABBITMQ_USER}
        password = "guest"
        password = ${?RABBITMQ_PASSWORD}
        virtualHost = "/"
        virtualHost = ${?RABBITMQ_VIRTUAL_HOST}
    }
    solace {
        initialContextFactory = "com.solacesystems.jndi.SolJNDIInitialContextFactory"
        initialContextFactory = ${?SOLACE_INITIAL_CONTEXT_FACTORY}
        connectionFactory = "/jms/cf/default"
        connectionFactory = ${?SOLACE_CONNECTION_FACTORY}
        url = "smf://solaceserver:55555"
        url = ${?SOLACE_URL}
        user = "admin"
        user = ${?SOLACE_USER}
        password = "admin"
        password = ${?SOLACE_PASSWORD}
        vpnName = "default"
        vpnName = ${?SOLACE_VPN}
    }
}

kafka {
    kafkaAccount {
        kafka.bootstrap.servers = "kafka:29092"
        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}
    }
}
